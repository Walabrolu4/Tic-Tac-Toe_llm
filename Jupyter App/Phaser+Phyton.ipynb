{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_W_eA3wyWyRw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#\n",
        "Colab Setup:\n",
        "1. Create a new Colab notebook.\n",
        "2. Install necessary Python packages:\n",
        "\n"
      ],
      "metadata": {
        "id": "LO4ULZvOsIuX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x_GY7MAYrtrc",
        "outputId": "889d9b99-07c6-48e0-d979-31224dad0b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.11/dist-packages (5.0.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install flask flask-cors pyngrok google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries and configure your LLM API key (using Colab's Secrets manager is recommended for keys):"
      ],
      "metadata": {
        "id": "ylnwS11stZwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata # For secrets\n",
        "import json\n",
        "# Assuming you've stored your API key as 'GEMINI_API_KEY' in Colab secrets\n",
        "try:\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "    genai.configure(api_key=api_key)\n",
        "    print(\"Gemini API Key configured.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"ERROR: Secret 'GEMINI_API_KEY' not found. Please add it in Colab's Secrets manager (key icon on the left).\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during API key configuration: {e}\")\n",
        "\n",
        "# Placeholder for the model\n",
        "llm_model = None\n",
        "if 'api_key' in locals() and api_key:\n",
        "     try:\n",
        "         llm_model = genai.GenerativeModel('gemini-1.5-flash') # Or another suitable model\n",
        "         print(\"Gemini model initialized.\")\n",
        "     except Exception as e:\n",
        "         print(f\"Error initializing Gemini model: {e}\")\n",
        "else:\n",
        "    print(\"Skipping model initialization due to missing API key.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn7M4yY3tS2q",
        "outputId": "1aff89b6-ca11-4115-f7fa-45b65dd89842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API Key configured.\n",
            "Gemini model initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup Imports and Run Flask Server\n",
        "\n",
        "## setup imports"
      ],
      "metadata": {
        "id": "zD6q_4K9SzdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify, send_from_directory\n",
        "from flask_cors import CORS\n",
        "import threading\n",
        "import os\n",
        "from pyngrok import ngrok # Import ngrok\n",
        "import re # For parsing LLM responses\n",
        "!mkdir -p game_files"
      ],
      "metadata": {
        "id": "Xte0nX6hSzLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flask App Setup"
      ],
      "metadata": {
        "id": "maG5NehLTYSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__, static_folder = 'game_files')\n",
        "CORS(app)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HIh92exTbfz",
        "outputId": "6466c864-b043-4726-c343-9366f8d3e3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<flask_cors.extension.CORS at 0x7ca5cf1eb9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Game Logic and placeholder"
      ],
      "metadata": {
        "id": "Xb2avmJNVfmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Game Logic Placeholder (will be refined) ---\n",
        "# Store game state server-side IF needed, or manage fully client-side and send state with each request\n",
        "# For this POC, let's assume the client (Phaser) manages state and sends it"
      ],
      "metadata": {
        "id": "RDBLTHcRVm2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Interaction Function"
      ],
      "metadata": {
        "id": "11snrg-zVqT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_decision(game_state, available_actions, user_instruction):\n",
        "    if not llm_model:\n",
        "         return {\"error\": \"LLM model not initialized.\"}\n",
        "\n",
        "    # --- Crucial: Prompt Engineering ---\n",
        "    # Provide context, rules, state, available actions, and the user's goal.\n",
        "    # Ask for the output in a specific, parseable format (like JSON).\n",
        "    prompt = f\"\"\"You are playing a game. Respond with ONLY the chosen action in the format specified.\n",
        "\n",
        "    Game: Tic-Tac-Toe\n",
        "    Your Mark: O (You are playing as O)\n",
        "    Player Mark: X\n",
        "\n",
        "    Current Board State:\n",
        "    {json.dumps(game_state['board'], indent=2)}\n",
        "    (Empty strings \"\" represent empty cells)\n",
        "\n",
        "    Available Actions (Choose one by coordinates row, col):\n",
        "    {json.dumps(available_actions, indent=2)}\n",
        "\n",
        "    User Instruction: \"{user_instruction}\"\n",
        "\n",
        "    Game Status: {game_state['status']}\n",
        "    Current Turn: {game_state['turn']}\n",
        "\n",
        "    Based on the user instruction and the current state, choose the best action from the available actions for player 'O'.\n",
        "    Respond ONLY with the chosen action as a JSON object like this: {{\"action\": \"place O at (row, col)\"}} or like this if you need coordinates: {{\"row\": r, \"col\": c}}\n",
        "    Example response for placing O in the top-left: {{\"row\": 0, \"col\": 0}}\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"--- Sending Prompt to LLM ---\")\n",
        "    # print(prompt) # Uncomment to debug the prompt\n",
        "\n",
        "    try:\n",
        "        response = llm_model.generate_content(prompt)\n",
        "        print(\"--- Received Response from LLM ---\")\n",
        "        print(response.text)\n",
        "\n",
        "        # --- Parse the LLM Response ---\n",
        "        # This is critical and might need robust error handling\n",
        "        try:\n",
        "            # Try parsing directly if LLM gives perfect JSON (unlikely)\n",
        "            # action_data = json.loads(response.text.strip())\n",
        "\n",
        "            # More robust: Search for JSON-like structure or extract coords\n",
        "            # Example naive extraction (adapt based on LLM output format):\n",
        "            import re\n",
        "            match = re.search(r'{\\s*\"row\":\\s*(\\d+),\\s*\"col\":\\s*(\\d+)\\s*}', response.text)\n",
        "            if match:\n",
        "                row, col = int(match.group(1)), int(match.group(2))\n",
        "                # Validate if this action is actually in available_actions\n",
        "                if {\"row\": row, \"col\": col} in available_actions:\n",
        "                    print(f\"LLM chose valid action: row={row}, col={col}\")\n",
        "                    return {\"row\": row, \"col\": col}\n",
        "                else:\n",
        "                     print(f\"LLM chose an INVALID action ({row},{col}), not in available actions.\")\n",
        "                     return {\"error\": f\"LLM chose an invalid action: ({row},{col})\"}\n",
        "\n",
        "            # Fallback or different parsing if needed\n",
        "            return {\"error\": \"Could not parse valid action from LLM response.\", \"raw_response\": response.text}\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "             print(f\"LLM response was not valid JSON: {response.text}\")\n",
        "             return {\"error\": \"LLM response was not valid JSON.\", \"raw_response\": response.text}\n",
        "        except Exception as e:\n",
        "             print(f\"Error parsing LLM response: {e}\")\n",
        "             return {\"error\": f\"Error parsing LLM response: {e}\", \"raw_response\": response.text}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling LLM API: {e}\")\n",
        "        return {\"error\": f\"Error calling LLM API: {e}\"}"
      ],
      "metadata": {
        "id": "fBNGxG3cVtI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Endpoint"
      ],
      "metadata": {
        "id": "_1xZzK4LWGLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route('/api/llm_move', methods=['POST'])\n",
        "def handle_llm_move():\n",
        "    data = request.json\n",
        "    game_state = data.get('gameState')\n",
        "    available_actions = data.get('availableActions') # JS needs to calculate these!\n",
        "    user_instruction = data.get('instruction')\n",
        "\n",
        "    if not game_state or not available_actions or user_instruction is None:\n",
        "        return jsonify({\"error\": \"Missing gameState, availableActions, or instruction\"}), 400\n",
        "\n",
        "    # Basic validation (more needed in a real app)\n",
        "    if game_state.get('currentPlayer') != 'O': # Assuming LLM plays 'O'\n",
        "         return jsonify({\"error\": \"Not LLM's turn\"}), 400\n",
        "    if game_state.get('status') != 'playing':\n",
        "         return jsonify({\"error\": \"Game is not active\"}), 400\n",
        "\n",
        "    decision = get_llm_decision(game_state, available_actions, user_instruction)\n",
        "    return jsonify(decision)"
      ],
      "metadata": {
        "id": "CJepvQUtWJnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Static File Serving"
      ],
      "metadata": {
        "id": "-npMBLdkWNcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Static File Serving ---\n",
        "# Create a directory in Colab's file system to hold your game files\n",
        "if not os.path.exists('game_files'):\n",
        "    os.makedirs('game_files')\n",
        "\n",
        "# You need to UPLOAD your index.html, game.js, phaser.min.js, etc.\n",
        "# into this 'game_files' directory using Colab's file browser (folder icon on left).\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    # Serves index.html from the 'game_files' directory\n",
        "    return send_from_directory('game_files', 'index.html')\n",
        "\n",
        "@app.route('/<path:filename>')\n",
        "def serve_static(filename):\n",
        "    # Serves other files (game.js, phaser.min.js, assets)\n",
        "    return send_from_directory('game_files', filename)\n"
      ],
      "metadata": {
        "id": "rXXZxo8oWQlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to Run Flask App"
      ],
      "metadata": {
        "id": "t5QH-gnGWVgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_flask():\n",
        "  # Needs to run on port 8080 for some Colab environments, or choose another\n",
        "  # Ngrok will tunnel to this port\n",
        "  print(\"Starting Flask server...\")\n",
        "  app.run(host='0.0.0.0', port=8880) # Run on all interfaces, port 8080"
      ],
      "metadata": {
        "id": "0c6TiCTYWX68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start ngrok Tunnel"
      ],
      "metadata": {
        "id": "a9FYy9cBWhBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def start_ngrok():\n",
        "    try:\n",
        "        # Terminate existing tunnels if any\n",
        "        ngrok.kill()\n",
        "        # Get ngrok auth token from Colab secrets if you have one (recommended for stable URLs)\n",
        "        try:\n",
        "             ngrok_auth_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "             ngrok.set_auth_token(ngrok_auth_token)\n",
        "             print(\"Ngrok auth token set.\")\n",
        "        except userdata.SecretNotFoundError:\n",
        "             print(\"INFO: NGROK_AUTH_TOKEN secret not found. Using ngrok without auth token (temporary URL).\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error setting ngrok auth token: {e}\")\n",
        "\n",
        "        # Start an HTTP tunnel on the same port Flask is running on\n",
        "        public_url = ngrok.connect(8880, \"http\")\n",
        "        print(f\" * ngrok tunnel available at: {public_url}\")\n",
        "        return public_url\n",
        "    except Exception as e:\n",
        "        print(f\"Error starting ngrok: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "ApvgNIW1WjZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Execution Logic"
      ],
      "metadata": {
        "id": "T3wArGg1WsOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "public_url = start_ngrok()\n",
        "\n",
        "print(\"\\n--- Server Setup Complete ---\")\n",
        "if public_url:\n",
        "    print(f\"Access your game POC at: {public_url}\")\n",
        "    print(f\"LLM endpoint will be at: {public_url}/api/llm_move\") # Print full URL\n",
        "else:\n",
        "    print(\"Failed to start ngrok tunnel. Server might be running but not accessible publicly.\")\n",
        "    # Maybe exit or raise error if ngrok fails?\n",
        "\n",
        "print(\"Make sure you have uploaded your index.html, game.js, and phaser.min.js into the 'game_files' directory in Colab.\")\n",
        "\n",
        "\n",
        "# --- Run Flask in Foreground (BLOCKING) ---\n",
        "# This will block the cell from 'finishing' until you manually interrupt it (e.g., Runtime -> Interrupt execution).\n",
        "print(\"\\n--- Starting Flask Server (Foreground - Blocking) ---\")\n",
        "print(f\"Flask is now running and listening on port 8880.\")\n",
        "print(\"Logs should appear directly below as requests come in.\")\n",
        "print(\"!!! The Colab cell will appear 'busy'. To stop the server, you MUST interrupt the kernel !!!\")\n",
        "try:\n",
        "    # Run Flask on 0.0.0.0 to accept connections from ngrok\n",
        "    app.run(host='0.0.0.0', port=8880, debug=False) # Turn debug=True for more Flask logs if needed, but can be verbose\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n--- Flask server stopped (KeyboardInterrupt) ---\")\n",
        "\n",
        "# The code below this line will NOT execute until the server is stopped.\n",
        "print(\"--- Flask server has been shut down ---\")\n",
        "\n",
        "\n",
        "'''\n",
        "# Start Flask in a separate thread so it doesn't block Colab execution\n",
        "flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
        "flask_thread.start()\n",
        "\n",
        "# Start ngrok and print the public URL\n",
        "public_url = start_ngrok()\n",
        "\n",
        "print(\"\\n--- Server Setup Complete ---\")\n",
        "if public_url:\n",
        "    print(f\"Access your game POC at: {public_url}\")\n",
        "else:\n",
        "    print(\"Failed to start ngrok tunnel. Server might be running but not accessible publicly.\")\n",
        "print(\"Flask server is running in the background.\")\n",
        "print(\"LLM endpoint is available at /api/llm_move (relative to the ngrok URL)\")\n",
        "print(\"Make sure you have uploaded your index.html, game.js, and phaser.min.js into the 'game_files' directory in Colab.\")\n",
        "# Keep the Colab cell running..\n",
        "'''"
      ],
      "metadata": {
        "id": "xcCGmv2RWuXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXTRA SHIT"
      ],
      "metadata": {
        "id": "_W_eA3wyWyRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from flask import Flask, request, jsonify, send_from_directory\n",
        "from flask_cors import CORS\n",
        "import threading\n",
        "import os\n",
        "from pyngrok import ngrok # Import ngrok\n",
        "\n",
        "# --- Flask App Setup ---\n",
        "app = Flask(__name__, static_folder='game_files') # Point static folder\n",
        "CORS(app) # Enable Cross-Origin Resource Sharing\n",
        "\n",
        "# --- Game Logic Placeholder (will be refined) ---\n",
        "# Store game state server-side IF needed, or manage fully client-side and send state with each request\n",
        "# For this POC, let's assume the client (Phaser) manages state and sends it.\n",
        "\n",
        "# --- LLM Interaction Function ---\n",
        "def get_llm_decision(game_state, available_actions, user_instruction):\n",
        "    if not llm_model:\n",
        "         return {\"error\": \"LLM model not initialized.\"}\n",
        "\n",
        "    # --- Crucial: Prompt Engineering ---\n",
        "    # Provide context, rules, state, available actions, and the user's goal.\n",
        "    # Ask for the output in a specific, parseable format (like JSON).\n",
        "    prompt = f\"\"\"You are playing a game. Respond with ONLY the chosen action in the format specified.\n",
        "\n",
        "    Game: Tic-Tac-Toe\n",
        "    Your Mark: O (You are playing as O)\n",
        "    Player Mark: X\n",
        "\n",
        "    Current Board State:\n",
        "    {json.dumps(game_state['board'], indent=2)}\n",
        "    (Empty strings \"\" represent empty cells)\n",
        "\n",
        "    Available Actions (Choose one by coordinates row, col):\n",
        "    {json.dumps(available_actions, indent=2)}\n",
        "\n",
        "    User Instruction: \"{user_instruction}\"\n",
        "\n",
        "    Game Status: {game_state['status']}\n",
        "    Current Turn: {game_state['turn']}\n",
        "\n",
        "    Based on the user instruction and the current state, choose the best action from the available actions for player 'O'.\n",
        "    Respond ONLY with the chosen action as a JSON object like this: {{\"action\": \"place O at (row, col)\"}} or like this if you need coordinates: {{\"row\": r, \"col\": c}}\n",
        "    Example response for placing O in the top-left: {{\"row\": 0, \"col\": 0}}\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"--- Sending Prompt to LLM ---\")\n",
        "    # print(prompt) # Uncomment to debug the prompt\n",
        "\n",
        "    try:\n",
        "        response = llm_model.generate_content(prompt)\n",
        "        print(\"--- Received Response from LLM ---\")\n",
        "        print(response.text)\n",
        "\n",
        "        # --- Parse the LLM Response ---\n",
        "        # This is critical and might need robust error handling\n",
        "        try:\n",
        "            # Try parsing directly if LLM gives perfect JSON (unlikely)\n",
        "            # action_data = json.loads(response.text.strip())\n",
        "\n",
        "            # More robust: Search for JSON-like structure or extract coords\n",
        "            # Example naive extraction (adapt based on LLM output format):\n",
        "            import re\n",
        "            match = re.search(r'{\\s*\"row\":\\s*(\\d+),\\s*\"col\":\\s*(\\d+)\\s*}', response.text)\n",
        "            if match:\n",
        "                row, col = int(match.group(1)), int(match.group(2))\n",
        "                # Validate if this action is actually in available_actions\n",
        "                if {\"row\": row, \"col\": col} in available_actions:\n",
        "                    print(f\"LLM chose valid action: row={row}, col={col}\")\n",
        "                    return {\"row\": row, \"col\": col}\n",
        "                else:\n",
        "                     print(f\"LLM chose an INVALID action ({row},{col}), not in available actions.\")\n",
        "                     return {\"error\": f\"LLM chose an invalid action: ({row},{col})\"}\n",
        "\n",
        "            # Fallback or different parsing if needed\n",
        "            return {\"error\": \"Could not parse valid action from LLM response.\", \"raw_response\": response.text}\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "             print(f\"LLM response was not valid JSON: {response.text}\")\n",
        "             return {\"error\": \"LLM response was not valid JSON.\", \"raw_response\": response.text}\n",
        "        except Exception as e:\n",
        "             print(f\"Error parsing LLM response: {e}\")\n",
        "             return {\"error\": f\"Error parsing LLM response: {e}\", \"raw_response\": response.text}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling LLM API: {e}\")\n",
        "        return {\"error\": f\"Error calling LLM API: {e}\"}\n",
        "\n",
        "# --- API Endpoint ---\n",
        "@app.route('/api/llm_move', methods=['POST'])\n",
        "def handle_llm_move():\n",
        "    data = request.json\n",
        "    game_state = data.get('gameState')\n",
        "    available_actions = data.get('availableActions') # JS needs to calculate these!\n",
        "    user_instruction = data.get('instruction')\n",
        "\n",
        "    if not game_state or not available_actions or user_instruction is None:\n",
        "        return jsonify({\"error\": \"Missing gameState, availableActions, or instruction\"}), 400\n",
        "\n",
        "    # Basic validation (more needed in a real app)\n",
        "    if game_state.get('turn') != 'O': # Assuming LLM plays 'O'\n",
        "         return jsonify({\"error\": \"Not LLM's turn\"}), 400\n",
        "    if game_state.get('status') != 'playing':\n",
        "         return jsonify({\"error\": \"Game is not active\"}), 400\n",
        "\n",
        "    decision = get_llm_decision(game_state, available_actions, user_instruction)\n",
        "    return jsonify(decision)\n",
        "\n",
        "# --- Static File Serving ---\n",
        "# Create a directory in Colab's file system to hold your game files\n",
        "if not os.path.exists('game_files'):\n",
        "    os.makedirs('game_files')\n",
        "\n",
        "# You need to UPLOAD your index.html, game.js, phaser.min.js, etc.\n",
        "# into this 'game_files' directory using Colab's file browser (folder icon on left).\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    # Serves index.html from the 'game_files' directory\n",
        "    return send_from_directory('game_files', 'index.html')\n",
        "\n",
        "@app.route('/<path:filename>')\n",
        "def serve_static(filename):\n",
        "    # Serves other files (game.js, phaser.min.js, assets)\n",
        "    return send_from_directory('game_files', filename)\n",
        "\n",
        "# --- Function to Run Flask App ---\n",
        "def run_flask():\n",
        "  # Needs to run on port 8080 for some Colab environments, or choose another\n",
        "  # Ngrok will tunnel to this port\n",
        "  print(\"Starting Flask server...\")\n",
        "  app.run(host='0.0.0.0', port=8880) # Run on all interfaces, port 8080\n",
        "\n",
        "# --- Start ngrok tunnel ---\n",
        "def start_ngrok():\n",
        "    try:\n",
        "        # Terminate existing tunnels if any\n",
        "        ngrok.kill()\n",
        "        # Get ngrok auth token from Colab secrets if you have one (recommended for stable URLs)\n",
        "        try:\n",
        "             ngrok_auth_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "             ngrok.set_auth_token(ngrok_auth_token)\n",
        "             print(\"Ngrok auth token set.\")\n",
        "        except userdata.SecretNotFoundError:\n",
        "             print(\"INFO: NGROK_AUTH_TOKEN secret not found. Using ngrok without auth token (temporary URL).\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error setting ngrok auth token: {e}\")\n",
        "\n",
        "        # Start an HTTP tunnel on the same port Flask is running on\n",
        "        public_url = ngrok.connect(8880, \"http\")\n",
        "        print(f\" * ngrok tunnel available at: {public_url}\")\n",
        "        return public_url\n",
        "    except Exception as e:\n",
        "        print(f\"Error starting ngrok: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "# Start Flask in a separate thread so it doesn't block Colab execution\n",
        "flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
        "flask_thread.start()\n",
        "\n",
        "# Start ngrok and print the public URL\n",
        "public_url = start_ngrok()\n",
        "\n",
        "print(\"\\n--- Server Setup Complete ---\")\n",
        "if public_url:\n",
        "    print(f\"Access your game POC at: {public_url}\")\n",
        "else:\n",
        "    print(\"Failed to start ngrok tunnel. Server might be running but not accessible publicly.\")\n",
        "print(\"Flask server is running in the background.\")\n",
        "print(\"LLM endpoint is available at /api/llm_move (relative to the ngrok URL)\")\n",
        "print(\"Make sure you have uploaded your index.html, game.js, and phaser.min.js into the 'game_files' directory in Colab.\")\n",
        "# Keep the Colab cell running...\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyX2sTCl0ri3",
        "outputId": "28fc6df0-2aa9-43fa-ce7b-7be8cef36983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Flask server...\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Address already in use\n",
            "Port 8880 is in use by another program. Either identify and stop that program, or start the server with a different port.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok auth token set.\n",
            " * ngrok tunnel available at: NgrokTunnel: \"https://4526-34-125-119-161.ngrok-free.app\" -> \"http://localhost:8880\"\n",
            "\n",
            "--- Server Setup Complete ---\n",
            "Access your game POC at: NgrokTunnel: \"https://4526-34-125-119-161.ngrok-free.app\" -> \"http://localhost:8880\"\n",
            "Flask server is running in the background.\n",
            "LLM endpoint is available at /api/llm_move (relative to the ngrok URL)\n",
            "Make sure you have uploaded your index.html, game.js, and phaser.min.js into the 'game_files' directory in Colab.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://127.0.0.1:8080/"
      ],
      "metadata": {
        "id": "lTHo_bnq-pns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i :8080"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIM1RbjEA5jy",
        "outputId": "ac58bc73-ba21-488c-fa91-2c3dd3e7ed1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "node      7 root   21u  IPv6 565608      0t0  TCP *:8080 (LISTEN)\n",
            "node      7 root   26u  IPv6 567319      0t0  TCP 8d035afb8930:8080->172.28.0.1:43616 (ESTABLISHED)\n",
            "node      7 root   28u  IPv6 567489      0t0  TCP 8d035afb8930:8080->172.28.0.1:43632 (ESTABLISHED)\n",
            "node      7 root   30u  IPv6 593289      0t0  TCP 8d035afb8930:8080->172.28.0.1:52292 (ESTABLISHED)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_dBlttoA82Z",
        "outputId": "4793e7ab-7413-4a50-ce19-a0506f213ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: kill: (6) - No such process\n"
          ]
        }
      ]
    }
  ]
}